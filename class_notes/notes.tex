\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}

\textwidth=7.6in
\textheight=9.9in
\topmargin=-.9in
\headheight=0in
\headsep=.5in
\hoffset=-1.5in
\setlength\parindent{0pt}

\begin{document}

\begin{center}
    \textbf{Probabilistic Graphical Models Class Notes} \\[0.25ex]
    Calvin Walker
\end{center}
\textbf{Lecture 2: Bayesian Networks} \\[0.5ex]
\textbf{Structure} 
\begin{itemize}
    \item $G = (V, E)$ is a directed acyclic graph such that: 
    \begin{itemize}
        \item One node $i \in V$ for each random variable $X_i$
        \item Pa$^G_{X_i}$ denotes the parents of $X_i$
        \item NonDescendants$_{X_i}$ are variables that are not descendents of $X_i$
    \end{itemize}
    \item $G$ encodes the following local independencies \begin{equation*}
        I_{l}(G) = (X_i \perp \text{NonDescendants}_{X_i} | \text{Pa}^G_{X_i})\ \forall X_i
    \end{equation*}
    i.e. $X_i$ is conditionally independent of NonDescendants$_{X_i}$ given Pa$^G_{X_i}$
\end{itemize}
\begin{itemize}
    \item A distribuition $P$ factorizes according to $G$ if and only if \begin{equation*}
        P(X_i, \dots , X_n) = \prod_{i \in V}P(X_i | \text{Pa}^G_{X_i})
    \end{equation*}
    \item A \textbf{Bayesian Network} is a pair $B = (P, G)$ for which \begin{itemize}
        \item $P$ factorizes over $G$
        \item $P$ is a set of conditional probability distributions $P(X_i|\text{Pa}^G_{X_i})$
    \end{itemize}
    \item So $G$ provides a compact way to represent conditional independencies that hold under $P$
\end{itemize}
\textbf{Independence Maps}
\begin{itemize}
    \item Let $I(P) = \{(X \perp Y\ |\ Z)\}$ be the set of independence assertions that hold in $P$
    \item A BN structure $G$ is an I-map for a set of independencies $I$ if $I(G) \subseteq I$
    \item A BN structure $G$ is an I-map fpr $P$ is $G$ is an I-map for $I(P)$, i.e. $I(G) \subseteq I(P)$ \begin{itemize}
        \item Any independence asserted by $G$ must hold in $P$, but the converse is not necessarily true. $P$ may have additional independencies not reflected in $G$
        \item So while any conditional independency expressed by $G$ holds, the conditional dependencies expressed by $G$ hold for some distributions that factorize over $G$
    \end{itemize}
\end{itemize}
Representation Theorem: Given a BN structure $G$ and joint distribution $P$, $P$ factorizes $G$ if and only if $G$ is an I-map for $P$ \\[0.5ex]
Proof $(P \leftarrow Q)$: Let $T$ be a topological ordering on the nodes in $G$, and $v_i$ be the set of nodes appearing before $i$ in $T$, excluding Pa$^G_{X_i}$. From $I_{l}(G)$ we have that $\{X_i \perp X_{v_i}\ |\ \text{Pa}^G_{X_i}\}$. Since $I(G) \subseteq I(P)$, \begin{equation*}
    P(X_1, \dots , X_n) = \prod_{i \in T} P(X_i | X_{v_i}, \text{Pa}^G_{X_i}) = \prod_{i \in T} P(X_i | \text{Pa}^G_{X_i})
\end{equation*}
Active Trial: Let $G$ be a BN structure $X_1 \leftrightarrow \dots \leftrightarrow X_n$ be a trail in $G$, and $Z$ be a subset of observed variables. The trail is active, i.e. dependency/information flow given $Z$ if \begin{itemize}
    \item For every v-structure, $X_i$ or one of its descendents is in $Z$
    \item No other node along the trail is in $Z$
\end{itemize}
D-seperation: let $X, Y, Z$ be three sets of nodes in $G$\begin{itemize}
    \item $X$ and $Y$ are d-separated given $Z$ if there is no active trail between any node in $X$ to any node in $Y$ given $Z$
    \item I.e if d-sep$_G(X, Y\ |\ Z)$, then $(X \perp Y\ |\ Z)$
\end{itemize}
For a BN structure $G$, we define the global Markov independencies as the set of independencies that correspond to d-separation: \begin{equation*}
    I(G) = \{(X \perp X\ |\ Z : \text{d-sep}_G(X, Y | Z))\}
\end{equation*}
\textbf{Lecture 4: Factor Graphs, Gaussian Networks}
\begin{itemize}
    \item The Markov network $H$ does not make the structure of the distribuition explicit, i.e. maximum cliques vs. other complete graph subsets. 
    \item A \textbf{factor graph} is a bipartite undirected graph with variable nodes (oval) and factor nodes (square). Edges exist only between variable nodes and factor nodes 
    \item Each factor node is associated with a single potential, the scope of which is the variables that are the factor's neighbors 
    \item Boltzmann Distribution: \begin{itemize}
        \item We can rewrite a factor $\phi(D)$ as $\phi(D) = \exp (-\psi(D))$ where $\psi(D) = - \log \phi(D)$
        \item The factorized distribution then becomes: \begin{equation*}
            P(X_1, \dots , X_n) = \frac{1}{Z}\exp \bigg( -\sum_{k = 1}^{K} \psi_k (D_k)\bigg)
        \end{equation*}
        \item $\sum_{k = 1}^{K}\psi_k(D_k)$ is referred to as the ``free energy''
        \item Can do inference as energy minimization
    \end{itemize}
    \item Log-Linear Markov Networks with Features: \begin{itemize}
        \item A feature is a function $f: \text{Val}(D_i) \mapsto \mathbb{R}$
        \item A set of features $F = \{f_1(D_1) \dots f_K(D_M)\}$ where $D_i$ is a complete subgraph in $H$
        \item A set of weights $\{w_1, \dots , w_M\}$ such that \begin{equation*}
            P(X_1, \dots , X_n) \propto \exp \bigg(- \sum_{i = 1}^{M}w_if_i(D_i) \bigg)
        \end{equation*}
        \item Features and weights can be reused for different factors 
        \item Clasically, features we hand-designed and weights learned from data 
    \end{itemize}
    \item Gaussian Markov Random Fields: \begin{itemize}
        \item Consider a multivariate Gaussian density $p$ over $x = [x_1, \dots , x_n]^T$
        \item The density function is defined as: \begin{equation*}
            p(x) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp \bigg(\frac{-1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu)\bigg)
        \end{equation*}
        Where the term in the exponential can be expressed as: \begin{align*}
            \frac{-1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu) &= \frac{-1}{2}(x - \mu)^T \Lambda (x - \mu)\\
            &= \frac{-1}{2}(x^T \Lambda x - 2x^T \Lambda \mu + \mu^T \Lambda \mu)
        \end{align*}
        This is referred to as the cononical form where $\Lambda = \Sigma^{-1}$ is the information matrix and $\eta = \Lambda\mu$ is the information vector 
        \item The information for parametrization $x \sim \mathcal{N}^{-1}(\eta, \Lambda)$ can also be expressed as \begin{align*}
            p(x) &\propto \exp (-\frac{1}{2}\sum_{i}\Lambda_{ii}x_i^2 + 2\eta_i x_i) \exp(-\sum_{i, j : i \neq j }\Lambda_{ij}x_ix_j) \\
            &\vdots \\
            &= \prod_i \phi_i(x_i) \cdot \prod_{i,j:i\neq j} \phi_{ij}(x_i, x_j)
        \end{align*}
        \item Any Gaussian distribuition can be represented by a pairwise Markov network with quadradic node and edge potentials 
        \item Two nodes $x_i$ and $x_j$ have an edge in the GMRF only if $\Lambda_{ij} \neq 0$
        \item The structure of the information matrix $\Lambda$ directly encodes the Markov network graph structure 
    \end{itemize}
    \item Converting Bayesian Networks to Markov Networks \begin{itemize}
        \item Moralization coverts a BN to a Markov network 
        \item The moral graph $\mathcal{M}(G)$ of a BN structure is an undirected graph over $V$ that contains an edge between $X_i$ and $X_j$ if:\begin{itemize}
            \item there is a direct edge between them 
            \item they are parents of the same node 
        \end{itemize}
        \item Introduce one potential for each CPD $\phi_i(X_i, \text{Pa}_{X_i}^G) = P(X_i | \text{Pa}_{X_i}^G)$
        \item $\mathcal{M}(G)$ is a minimal I-map for $G$. If $G$ is moral, then $\mathcal{M}(G)$ is a perfect I-map for $G$
    \end{itemize}
    \item Converting a Markov network $H$ to a Bayesian network $G$ is harder, involves adding many edges \begin{itemize}
        \item An undirected graph is \textbf{chordal} if every cycle of length 3 has a shortcut between non-consecutive nodes 
        \item If $G$ is a minimal I-map for $H$, then $G$ must be chordal 
        \item Generating a BN for a Markov network involves triangulating the graph by adding edges to make the graph chordal 
        \item Triangulation results in a loss of independence relations present in $H$
    \end{itemize}
\end{itemize}
\textbf{Lecture 5: Conditional Random Fields}
\begin{itemize}
    \item A \textbf{generative} model requires representing the joint distribuition $P(X, Y) = P(X|Y)P(Y)$, since we can generate $X$ given label $Y$ 
    \item A \textbf{discriminative} model only requires a representation of the conditional distribuition $P(Y | X)$, so we can discriminate between different $Y$ without estimating $P(X)$
    \item Ex. Naive Bayes ($X_i \perp X_{-i} | Y$) vs. Logistic Regression: $P(Y = 1 | x; w) = \frac{1}{1 + e^{-z}}$ \begin{itemize}
        \item Every conditional distribution that can be represented via Naive Bayes can also be represented using the logistic model 
        \item Ignoring dependencies might double-count evidence, i.e. spam classification and two words that always appear together (but are assumed independent)
    \end{itemize}
    \item Tradeoffs between Generative vs. Discriminative Models: \begin{itemize}
        \item Missing data: Generative allow marginalization over unseen variables, e.g. $X = \{X_O, X_U\}, \\ P(Y|X_O) = \frac{\big(\sum_{X_U} P(X_O, X_U | Y)\big)P(Y)}{\sum_Y \sum_{X_U}P(X_O, X_U | Y) P(Y)}$ Discriminative models typically require all $X$ be observed 
        \item Unlabeled data: Relativley easy with generative models, but difficult with discriminative models. 
        \item Adding new classes: Generative models train class-conditioned distributions separatley. Discriminative models have interactions between parameters
        \item Calibrated Probabilities: Discriminative models typically yield more accurate probablities. Generative models can be overconfident due to independence assumptions 
        \item Data-dependent models: Discriminative models allow us to vary the model according to the data. Generative models employ data-independent parameterizations 
    \end{itemize}
    \item MLE of generative models is more efficient than training discriminative models, but may have higher asymptotic error. 
\end{itemize}
\textbf{Conditional Random Fields}: 
\begin{itemize}
    \item Undirected graph with nodes for $Y$ and $X$ (alt. partially directed, with $X$ the parent of $Y$)
    \item Parametrized by a set of factors $\phi_1(D_1), \dots , \phi_m(D_m)$
    \item Represent conditional distribution $P(Y|X)$ rather than the joint 
    \item Avoid representing dist. over $X$, so no potientials involving only $X$ \begin{align*}
        P(Y|X) &= \frac{1}{Z(X)}\prod_{i = 1}^{m}\phi_i(D_i) \\
        Z(X) &= \sum_{Y}\prod_{i = 1}^{m}\phi_i(D_i)
    \end{align*}
    \item Just like Markov network, except the partition function depends on the observed variables $X$
    \item The conditional distribution factiorizes as: \begin{align*}
        P(Y|X) &= \frac{1}{Z(X)}\prod_{i = 1}^{k - 1}\phi_i(Y_i, Y_{i + 1})\prod_{i = 1}^{k}\phi(Y_i, X_i) \\
        Z(X) &= \sum_{Y}\prod_{i = 1}^{k - 1}\phi_i(Y_i, Y_{i + 1})\prod_{i = 1}^{k}\phi(Y_i, X_i)
    \end{align*}
    \item Hidden Markov Model: widely used to model sequential random variables \begin{equation*}
        P(X, Y) = \prod_{t = 1}^{T}P(Y_t | Y_{t - 1})P(X_t | Y_t)
    \end{equation*} Which requires specifying the generative model. Instead, construct discriminative version by reversing direction of arrows, ex. Maximum Entropy Markov Model: \begin{equation*}
        P(Y|X) = \prod_{t = 1}^{T}P(Y_t|Y_{t - 1},X_1, X_2, \dots X_T, X_g)
    \end{equation*}
    Where $X_g$ are global features. Suffers from label bias problem: observations at time $t$ do not influence states prior to $t$ per DAG structure. The Chain-Structured CRF version, uses undirected edges, which yields the model: 
    \begin{equation*}
        P(Y|X) = \frac{1}{Z(X)}\prod_{t = 1}^T \psi(Y_t|X) \prod_{t - 1}^{T - 1}\phi(Y_t, Y_{t + 1}| X)
    \end{equation*} Requires the entire observation
    \item Naive Markov Model: Assume $X$ and $Y$ are related by the following factors: $\phi_0(Y) = \exp\{w_0 \mathbf{1}[Y = 1]\}$ and $\phi_i(X_i, Y) = \exp\{w_i \mathbf{1}[X_i = 1, Y = 1]\}$. So the conditional distribution becomes: \begin{align*}
        P(Y = 1|x_1, \dots, x_k) = \sigma \bigg( w_0 + \sum_{i = 1}^{k}w_ix_i \bigg)
    \end{align*}
    \item CRF Parametrization: Factors may depend on a large number of variables. Typically, parameterize factors using log-linear representation: \begin{equation*}
        \phi_c(X_c, Y_c) = \exp(w_c^T f_c(X_c, Y_c))
    \end{equation*}
\end{itemize}


\end{document}